# Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Paper Explained)

link
https://youtu.be/9dSkvxS2EB0?si=YB4OD407ou-H1r23

Transformers: every token depends on every other
RNN: token at time 'i' depends on the previous hidden state + actual token. 
S4: fix transform mtx, no nonlinearity: big mtx multiplication, non expressive. 
mamba: 
